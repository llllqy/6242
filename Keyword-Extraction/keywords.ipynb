{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "import numpy as np\n",
    "import math\n",
    "import csvkit\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download package; \n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "# run the code above just once\n",
    "\n",
    "def clean(text):\n",
    "    text = text.lower()\n",
    "    printable = set(string.printable)\n",
    "    text = filter(lambda x: x in printable, text)\n",
    "    text = \"\".join(list(text))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_key(input_text):\n",
    "    Cleaned_text = clean(input_text)\n",
    "    text = word_tokenize(Cleaned_text)\n",
    "\n",
    "    POS_tag = nltk.pos_tag(text)\n",
    "\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    adjective_tags = ['JJ','JJR','JJS']\n",
    "\n",
    "    lemmatized_text = []\n",
    "\n",
    "    for word in POS_tag:\n",
    "        if word[1] in adjective_tags:\n",
    "            lemmatized_text.append(str(wordnet_lemmatizer.lemmatize(word[0],pos=\"a\")))\n",
    "        else:\n",
    "            lemmatized_text.append(str(wordnet_lemmatizer.lemmatize(word[0])))\n",
    "    POS_tag = nltk.pos_tag(lemmatized_text)\n",
    "    stopwords = []\n",
    "\n",
    "    wanted_POS = ['NN','NNS','NNP','NNPS','JJ','JJR','JJS','VBG','FW'] \n",
    "\n",
    "    for word in POS_tag:\n",
    "        if word[1] not in wanted_POS:\n",
    "            stopwords.append(word[0])\n",
    "\n",
    "    punctuations = list(str(string.punctuation))\n",
    "\n",
    "    stopwords = stopwords + punctuations\n",
    "    stopword_file = open(\"long_stopwords.txt\", \"r\")\n",
    "    lots_of_stopwords = []\n",
    "\n",
    "    for line in stopword_file.readlines():\n",
    "        lots_of_stopwords.append(str(line.strip()))\n",
    "    stopword_file.close()\n",
    "\n",
    "    stopwords_plus = []\n",
    "    stopwords_plus = stopwords + lots_of_stopwords\n",
    "    stopwords_plus = set(stopwords_plus)\n",
    "    processed_text = []\n",
    "    \n",
    "    for word in lemmatized_text:\n",
    "        if word not in stopwords_plus:\n",
    "            processed_text.append(word)\n",
    "    vocabulary = list(set(processed_text))\n",
    "    vocab_len = len(vocabulary)\n",
    "\n",
    "    weighted_edge = np.zeros((vocab_len,vocab_len),dtype=np.float32)\n",
    "\n",
    "    score = np.zeros((vocab_len),dtype=np.float32)\n",
    "    window_size = 3\n",
    "    covered_coocurrences = []\n",
    "\n",
    "    for i in range(0,vocab_len):\n",
    "        score[i]=1\n",
    "        for j in range(0,vocab_len):\n",
    "            if j==i:\n",
    "                weighted_edge[i][j]=0\n",
    "            else:\n",
    "                for window_start in range(0,(len(processed_text)-window_size)):\n",
    "\n",
    "                    window_end = window_start+window_size\n",
    "\n",
    "                    window = processed_text[window_start:window_end]\n",
    "\n",
    "                    if (vocabulary[i] in window) and (vocabulary[j] in window):\n",
    "\n",
    "                        index_of_i = window_start + window.index(vocabulary[i])\n",
    "                        index_of_j = window_start + window.index(vocabulary[j])\n",
    "\n",
    "                        # index_of_x is the absolute position of the xth term in the window \n",
    "                        # (counting from 0) \n",
    "                        # in the processed_text\n",
    "\n",
    "                        if [index_of_i,index_of_j] not in covered_coocurrences:\n",
    "                            weighted_edge[i][j]+=1/math.fabs(index_of_i-index_of_j)\n",
    "                            covered_coocurrences.append([index_of_i,index_of_j])\n",
    "    inout = np.zeros((vocab_len),dtype=np.float32)\n",
    "\n",
    "    for i in range(0,vocab_len):\n",
    "        for j in range(0,vocab_len):\n",
    "            inout[i]+=weighted_edge[i][j]\n",
    "    MAX_ITERATIONS = 50\n",
    "    d=0.85\n",
    "    threshold = 0.0001 #convergence threshold\n",
    "\n",
    "    for iter in range(0,MAX_ITERATIONS):\n",
    "        prev_score = np.copy(score)\n",
    "\n",
    "        for i in range(0,vocab_len):\n",
    "            summation = 0\n",
    "            for j in range(0,vocab_len):\n",
    "                if weighted_edge[i][j] != 0:\n",
    "                    summation += (weighted_edge[i][j]/inout[j])*score[j]\n",
    "\n",
    "            score[i] = (1-d) + d*(summation)\n",
    "\n",
    "        if np.sum(np.fabs(prev_score-score)) <= threshold: #convergence condition\n",
    "            break\n",
    "    phrases = []\n",
    "    phrase = \" \"\n",
    "    for word in lemmatized_text:\n",
    "\n",
    "        if word in stopwords_plus:\n",
    "            if phrase!= \" \":\n",
    "                phrases.append(str(phrase).strip().split())\n",
    "            phrase = \" \"\n",
    "        elif word not in stopwords_plus:\n",
    "            phrase+=str(word)\n",
    "            phrase+=\" \"\n",
    "    unique_phrases = []\n",
    "\n",
    "    for phrase in phrases:\n",
    "        if phrase not in unique_phrases:\n",
    "            unique_phrases.append(phrase)\n",
    "\n",
    "    for word in vocabulary:\n",
    "        #print word\n",
    "        for phrase in unique_phrases:\n",
    "            if (word in phrase) and ([word] in unique_phrases) and (len(phrase)>1):\n",
    "                #if len(phrase)>1 then the current phrase is multi-worded.\n",
    "                #if the word in vocabulary is present in unique_phrases as a single-word-phrase\n",
    "                # and at the same time present as a word within a multi-worded phrase,\n",
    "                # then I will remove the single-word-phrase from the list.\n",
    "                unique_phrases.remove([word])\n",
    "  \n",
    "    phrase_scores = []\n",
    "    keywords = []\n",
    "    \n",
    "    for phrase in unique_phrases:\n",
    "        phrase_score=0\n",
    "        keyword = ''\n",
    "        for word in phrase:\n",
    "            keyword += str(word)\n",
    "            keyword += \" \"\n",
    "            phrase_score+=score[vocabulary.index(word)]\n",
    "        phrase_scores.append(phrase_score)\n",
    "        keywords.append(keyword.strip())\n",
    "\n",
    "    i=0\n",
    "    sorted_index = np.flip(np.argsort(phrase_scores),0)\n",
    "\n",
    "    keywords_num = min(len(keywords),5)\n",
    "    return keywords[:keywords_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the duplicate articles from the origin dataset\n",
    "dt=pd.read_csv(\"0_arxiv_affil.csv\")\n",
    "before_tit=''\n",
    "i = 0\n",
    "for row in dt['title']:\n",
    "    if before_tit == row:\n",
    "        dt=dt.drop([i])\n",
    "    else:\n",
    "        before_tit = row\n",
    "    i = i + 1\n",
    "    print(i)\n",
    "dt.to_csv(\"1_arxiv_affil_no_repli.csv\",index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run for the keyword extracting\n",
    "dt=pd.read_csv(\"1_arxiv_affil_no_repli.csv\")\n",
    "data_keyword=[]\n",
    "i=0\n",
    "for row in dt['title']:\n",
    "    extract=extract_key(row)\n",
    "    data_keyword.append(extract)\n",
    "    i=i+1\n",
    "    print(i)\n",
    "#     print(extract)\n",
    "dt['keyword'] = data_keyword\n",
    "dt.to_csv(\"2_output.csv\",index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the column which keyword is []\n",
    "data=pd.read_csv(\"2_output.csv\")\n",
    "i = 0\n",
    "for row in dt['keyword']:\n",
    "    if row == \"[]\":\n",
    "        data=data.drop([i])\n",
    "    i = i + 1\n",
    "    print(i)\n",
    "data.to_csv(\"3_output_noNone.csv\",index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add id\n",
    "data=pd.read_csv(\"3_output_noNone.csv\")\n",
    "id = []\n",
    "i=0\n",
    "for row in dt['title']:\n",
    "    id.append(i)\n",
    "    i=i+1\n",
    "#     print(extract)\n",
    "\n",
    "dt['id'] = id\n",
    "dt.to_csv(\"4_output_withid.csv\",index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting the matching part\n",
    "data=pd.read_csv(\"4_output_withid.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MD(array1,array2):#1对于2的匹配度\n",
    "    m=len(set(array1) & set(array2))\n",
    "    md = m/len(set(array2))\n",
    "    return md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def takeSecond(elem):\n",
    "    return elem[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relatePaper=[]\n",
    "index=0\n",
    "while index<4000:\n",
    "    i=0\n",
    "    this_rlpp = []\n",
    "    this_paper = dt.iloc[index][\"keyword\"]\n",
    "#     print(dt.iloc[index][\"title\"])\n",
    "    this_field = dt.iloc[index][\"categories\"]\n",
    "    for row in dt['title']:\n",
    "        if (i!=index) & (dt.iloc[i][\"categories\"] == this_field):\n",
    "            match_degree=MD(row,this_paper)\n",
    "            if (match_degree>=0.5):\n",
    "                this_rlpp.append([i,match_degree])\n",
    "#                 print(row +\" \"+dt.iloc[i][\"title\"])\n",
    "        i=i+1\n",
    "        if len(this_rlpp)>5:\n",
    "            break;\n",
    "    this_rlpp.sort(key=takeSecond,reverse=True)\n",
    "    rlpp=this_rlpp[:3]#filter the top 3\n",
    "    relatePaper.append(rlpp)\n",
    "    index = index+1\n",
    "    print(index)\n",
    "dt['relatePaper'] = relatePaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.to_csv(\"final_output.csv\",index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
